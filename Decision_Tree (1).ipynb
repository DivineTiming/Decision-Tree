{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoritical Questions"
      ],
      "metadata": {
        "id": "vcz3QjBnecZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:**\n",
        "\n",
        "What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "**Answer 1:**\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n",
        "\n",
        "In classification, it splits a dataset into subsets based on feature values, using a tree-like model of decisions. Each internal node represents a test on a feature, each branch corresponds to an outcome of the test, and each leaf node holds a class label.\n",
        "\n",
        "The tree recursively splits the data so that samples with the same class label are grouped together, making predictions by traversing the path from root to leaf according to feature values of the instance being classified.\n",
        "\n",
        "Decision Tree works by partitioning the data based on a series of questions or rules:\n",
        "\n",
        "1. Start at the Root Node: The process begins with the entire dataset at the top of the tree, known as the \"root node\".\n",
        "\n",
        "2. Splitting the Data: The algorithm evaluates different features (attributes) to find the best way to split the data into two or more homogeneous subsets. The goal is to maximize the purity of the resulting subsets—meaning that each subset contains data points primarily belonging to a single class. Common metrics used to determine the \"best\" split include Information Gain, Gini Impurity, or Chi-squared.\n",
        "\n",
        "3. Creating Decision Nodes: The feature that provides the optimal split becomes a \"decision node\" (or internal node). Branches extending from this node represent the possible outcomes of the test or different values of that feature.\n",
        "\n",
        "4. Repeating the Process (Recursion): Steps 2 and 3 are repeated recursively for each new subset. This process continues until a stopping criterion is met, such as:\n",
        "\n",
        " - All data points in a subset belong to the same class.\n",
        " - No more features are available for splitting.\n",
        " - The maximum allowed depth of the tree is reached.\n",
        "\n",
        "5. Reaching Leaf Nodes: When the stopping criteria are met, the final nodes are called \"leaf nodes\" (or terminal nodes). Each leaf node represents the final class label or a decision outcome for that specific branch's path.\n",
        "6. Making Predictions: To classify a new data point, it traverses the tree starting from the root, following the branches corresponding to its feature values until it reaches a leaf node. The class label of that leaf node is the predicted classification for the data point.\n",
        "\n",
        "Effectively, a decision tree asks a series of \"if-then-else\" questions to navigate from the general population to a specific classification, making it a highly intuitive and easily interpretable model.\n",
        "\n",
        "\n",
        "\n",
        "**Question 2:**\n",
        "\n",
        "Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Answer 2:**\n",
        "\n",
        "Gini Impurity and Entropy are impurity measures used in decision trees to determine the best split at each node. Both metrics quantify the \"mixedness\" or \"disorder\" of data within a node, with a score of 0 indicating a \"pure\" node (all data points belong to one class) and higher scores indicating greater impurity. Decision trees use these measures to find splits that maximize the reduction in impurity, also known as information gain.\n",
        "\n",
        "Gini Impurity measures the probability of an element being incorrectly classified if it were randomly assigned a label based on the class distribution of the node.\n",
        "\n",
        "A Gini Index of 0 means a perfectly pure node, while a value of  (for a two-class problem) indicates maximum impurity, where classes are split equally.\n",
        "\n",
        "The algorithm selects the split that results in the lowest Gini Impurity for the child nodes. Lower impurity means the split is better at separating classes.\n",
        "\n",
        "Entropy quantifies the amount of uncertainty or disorder in the dataset. It measures how much a node's class distribution deviates from a pure distribution.\n",
        "\n",
        "A node with an Entropy of 0 is pure, and the highest value (example, 1 for a two-class problem with equal probability) represents maximum impurity.\n",
        "\n",
        "Application in trees: The algorithm chooses the split that provides the greatest reduction in entropy from the parent node to the child nodes. This reduction is known as \"information gain\".\n",
        "\n",
        "Impact on Decision Tree Splits\n",
        "\n",
        "The primary goal of both measures is to guide the decision tree to make the most effective splits at each step.\n",
        "\n",
        "The algorithm evaluates all possible splits for all features and chooses the one that results in the greatest reduction in impurity (highest information gain).\n",
        "\n",
        "This process is repeated recursively at each child node until a stopping criterion is met, such as the nodes becoming pure or reaching a predefined maximum depth.\n",
        "\n",
        "Both measures are effective, but Gini Impurity is generally faster to compute because it avoids logarithmic calculations.\n",
        "For most datasets, the results from both are very similar, making Gini Impurity a popular choice, especially for large datasets where computational speed is a priority.\n",
        "\n",
        "\n",
        "\n",
        "**Question 3:**\n",
        "\n",
        "What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "**Answer 3:**\n",
        "\n",
        "Pre-Pruning halts the tree-growing process early by setting limits (like maximum depth or minimum samples per split), while Post-Pruning removes nodes from a fully grown tree that don't provide empirical improvement, often based on validation set performance.\n",
        "\n",
        "Pre-Pruning is computationally faster and helps in handling large datasets early. Post-Pruning is usually more accurate in preventing overfitting by examining fully constructed trees and trimming unnecessary nodes\n",
        "\n",
        "Pre-pruning stops a decision tree from growing further during training by setting limits like maximum depth, while post-pruning first builds a complete tree and then removes branches that do not improve accuracy. A practical advantage of pre-pruning is its computational efficiency, as it avoids the overhead of building a full tree, whereas a key advantage of post-pruning is that it can lead to better pruning decisions because it evaluates the tree's overall structure before removing branches.\n",
        "\n",
        "Pre-pruning\n",
        "\n",
        "• Definition: Also called early stopping, it involves setting constraints before or during the tree's growth to prevent it from becoming too complex.\n",
        "• How it works: The algorithm is stopped from creating new branches if certain conditions are met, such as a maximum depth or a minimum number of samples required in a leaf node.\n",
        "• Practical advantage: Computational efficiency. It is faster because it does not need to build the full, potentially large, tree before starting the pruning process.\n",
        "\n",
        "Post-pruning\n",
        "\n",
        "• Definition: Also known as backward pruning, it involves building a complete, unpruned tree first and then trimming it afterward.\n",
        "• How it works: Branches are removed or converted into a leaf node if they do not contribute significantly to the model's accuracy, often by using a cross-validation set to check performance.\n",
        "• Practical advantage: Potentially more accurate pruning. It can make better decisions about removing branches because it considers the entire structure of the fully grown tree, avoiding the risk of cutting off potentially useful parts too early.\n",
        "\n",
        "\n",
        "\n",
        "**Question 4:**\n",
        "\n",
        "What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "**Answer 4:**\n",
        "\n",
        "Information Gain measures the change in entropy after a dataset is split on an attribute. It quantifies how much uncertainty is reduced by a split, calculated as the difference between the entropy of the parent node and the weighted sum of entropies of child nodes. Information Gain is crucial because the split with the highest value leads to the purest (most informative) child nodes and is thus selected for the next split, driving the construction of an effective, accurate tree.\n",
        "\n",
        "Information gain measures how much a split on a feature reduces the uncertainty (entropy) in a dataset. It is important for choosing the best split because decision trees aim to maximize information gain, selecting the feature that results in the purest possible child nodes with the highest reduction in disorder.\n",
        "\n",
        "Information gain quantifies the expected reduction in entropy (or disorder) of a dataset after it's split based on a particular attribute.\n",
        "\n",
        " - Entropy: Entropy is a measure of a dataset's impurity or randomness. A dataset with high entropy is mixed with different classes, while a dataset with low entropy is more pure and contains fewer mixed classes.\n",
        "\n",
        " - Calculates the difference: It is calculated by subtracting the weighted average entropy of the child nodes from the entropy of the parent node. A higher information gain means the split is more effective.\n",
        "\n",
        "Why it's important for choosing the best split :\n",
        "\n",
        " - Feature selection: Decision trees use information gain to select the best feature to split on at each node.\n",
        "\n",
        " - Maximizes purity: The algorithm chooses the feature with the highest information gain, as this feature provides the most information about the target variable and creates the purest possible subsets of data.\n",
        "\n",
        " - Guides the tree's structure: By consistently choosing the split with the highest information gain at each step, the tree is built in a way that efficiently separates the data into more homogeneous groups, leading to more accurate predictions.\n",
        "\n",
        "\n",
        "\n",
        "**Question 5:**\n",
        "\n",
        "What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "**Answer 5:**\n",
        "\n",
        "Common applications of decision trees include medical diagnosis, fraud detection, loan approval, and customer churn prediction. The main advantages are their ease of interpretation and visualization, ability to handle both numerical and categorical data, and minimal data preprocessing requirements. However, a key limitation is their tendency to overfit the training data, especially with complex trees.\n",
        "\n",
        "Common applications :\n",
        "\n",
        " - Healthcare: Diagnosing diseases by analyzing patient symptoms, medical history, and test results to guide treatment plans.\n",
        "\n",
        " - Fraud detection: Identifying fraudulent transactions based on historical patterns.\n",
        "\n",
        " - Loan approval: Assessing credit risk by evaluating factors like credit score, income, and loan history.\n",
        "\n",
        " - Customer churn prediction: Identifying customers likely to leave based on behavior and purchase history.\n",
        "\n",
        " - Customer segmentation: Grouping customers for targeted marketing campaigns.\n",
        "\n",
        " - Education: Predicting student performance based on attendance and past grades.\n",
        "\n",
        " - Retail: Predicting sales trends for inventory management.\n",
        "\n",
        " - Telecommunications: Predicting customer churn.\n",
        "\n",
        "Advantages :\n",
        "\n",
        " - Easy to interpret: The logic is straightforward and can be visualized, making it easy for non-experts to understand.\n",
        "\n",
        " - Little data preparation: They require less data cleaning and can handle both numerical and categorical data without much preprocessing.\n",
        "\n",
        " - Handles missing values: Many algorithms can work with incomplete data without needing imputation.\n",
        "\n",
        " - Versatile: Can be used for both classification and regression tasks.\n",
        "\n",
        "Limitations :\n",
        "\n",
        " - Overfitting: Decision trees can become very complex, leading them to fit the training data too closely and perform poorly on new data. This is a significant problem, especially with small datasets.\n",
        "\n",
        " - Instability: Small changes in the data can lead to a completely different tree being generated.\n",
        "\n",
        " - Bias: They can be biased toward features with more levels.\n",
        "\n",
        " - Difficult for complex relationships: For tasks with complex relationships, decision trees may not be the most accurate model.\n",
        "\n"
      ],
      "metadata": {
        "id": "m03rGaKaefla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "9cJB8nsxleWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Loading data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree with Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Feature importances\n",
        "print(\"Feature importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QssslhZlhrG",
        "outputId": "f851e119-1259-4aae-f35f-1486bb4fd4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7\n",
        "\n",
        "# Fully-grown tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# max_depth=3\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "acc_pruned = accuracy_score(y_test, clf_pruned.predict(X_test))\n",
        "\n",
        "print(\"Accuracy full tree:\", acc_full)\n",
        "print(\"Accuracy max_depth=3:\", acc_pruned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V16bNzrmC7V",
        "outputId": "a1272325-d82d-45b1-c667-d454f3105a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy full tree: 1.0\n",
            "Accuracy max_depth=3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b283766a",
        "outputId": "f9cc897e-4260-40e2-b667-173f57f8d960"
      },
      "source": [
        "# 8\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "X, y = data, target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature importances:\", reg.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 10.416078431372549\n",
            "Feature importances: [5.12956739e-02 3.35270585e-03 5.81619171e-03 2.27940651e-06\n",
            " 2.71483790e-02 6.00326256e-01 1.36170630e-02 7.06881622e-02\n",
            " 1.94062297e-03 1.24638653e-02 1.10116089e-02 9.00872742e-03\n",
            " 1.93328464e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca13460d",
        "outputId": "7887fbb2-6778-4e2d-b37f-1bc22d08a30d"
      },
      "source": [
        "# 9\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Loading the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Defining the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Instantiating a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Setting up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(\"Parameter grid defined and GridSearchCV set up successfully.\")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Grid search completed.\")\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get the best estimator from GridSearchCV\n",
        "best_clf = grid_search.best_estimator_\n",
        "\n",
        "# Making predictions on the test set using the best estimator\n",
        "y_pred_test = best_clf.predict(X_test)\n",
        "\n",
        "# Calculating the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Test set accuracy with best parameters: {test_accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (120, 4)\n",
            "X_test shape: (30, 4)\n",
            "y_train shape: (120,)\n",
            "y_test shape: (30,)\n",
            "Parameter grid defined and GridSearchCV set up successfully.\n",
            "Grid search completed.\n",
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best cross-validation score: 0.9417\n",
            "Test set accuracy with best parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10 :**\n",
        "\n",
        "Imagine you're working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "**Answer 10 :**\n",
        "\n",
        " - Data Loading and Initial Exploration: Loading the dataset and understanding its structure, identifying data types and getting an initial overview of missing values and categorical features\n",
        "\n",
        " - Handle Missing Values: Implementing appropriate strategies for handling missing values which may include imputation (e.g., mean, median for numerical; mode for categorical) or removal, based on the nature and extent of missingness.\n",
        "\n",
        " - Encoding Categorical Features: Converting categorical features into a numerical format suitable for machine learning models. This could involve one-hot encoding, label encoding, or other relevant techniques, considering the cardinality of features.\n",
        "\n",
        " - Splitting Data into Training and Testing Sets: Dividing the preprocessed dataset into training and testing sets to prepare for model development and unbiased evaluation.\n",
        "\n",
        " - Training a Decision Tree Model: Instantiate and train a Decision Tree Classifier on the training data using default parameters initially.\n",
        "\n",
        " - Tuning Hyperparameters with GridSearchCV: Defining a parameter grid for key Decision Tree hyperparameters (e.g., max_depth, min_samples_split, criterion) and use GridSearchCV with cross-validation to find the optimal combination of parameters.\n",
        "\n",
        " - Evaluating Model Performance: Evaluating the tuned Decision Tree model on the unseen test set using appropriate classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC, given the healthcare context.\n",
        "\n",
        " - Describing Business Value: Explaining the real-world business value this disease prediction model could provide for a healthcare company, focusing on aspects like early diagnosis, resource allocation, and patient outcomes. Such a model helps automate disease risk prediction, enabling early interventions, prioritizing patient care, reducing manual workload, and improving overall decision-making and resource allocation in healthcare.\n"
      ],
      "metadata": {
        "id": "Nd8wJh5go03B"
      }
    }
  ]
}